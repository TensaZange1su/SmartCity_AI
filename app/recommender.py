import numpy as np
import pandas as pd
from sentence_transformers import SentenceTransformer, CrossEncoder
from openai import OpenAI
from transformers import pipeline
import os
from dotenv import load_dotenv

load_dotenv()
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# === –û—Å–Ω–æ–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏ ===
embedder = SentenceTransformer("sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
cross_encoder = CrossEncoder("cross-encoder/ms-marco-MiniLM-L-6-v2")
intent_classifier = pipeline("zero-shot-classification", model="facebook/bart-large-mnli")

# === Intent-–∫–∞—Ç–µ–≥–æ—Ä–∏–∏ ===
INTENT_CATEGORY_MAP = {
    "night_activity": ["–ë–∞—Ä", "–ü–∞–±", "–ö–∞—Ä–∞–æ–∫–µ", "–ö–ª—É–±", "–†–∞–∑–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω–æ–µ –∑–∞–≤–µ–¥–µ–Ω–∏–µ", "–ú–µ—Å—Ç–∞ –æ—Ç–¥—ã—Ö–∞ / –†–∞–∑–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω—ã–µ –∑–∞–≤–µ–¥–µ–Ω–∏—è"],
    "entertainment": ["–ö–∏–Ω–æ—Ç–µ–∞—Ç—Ä", "–ö–≤–µ—Å—Ç", "–ë–æ—É–ª–∏–Ω–≥", "–¢–µ–∞—Ç—Ä", "–ú—É–∑–µ–π", "–ü–∞—Ä–∫", "–†–∞–∑–≤–ª–µ—á–µ–Ω–∏—è"],
    "nature_relax": ["–ü–∞—Ä–∫", "–ù–∞–±–µ—Ä–µ–∂–Ω–∞—è", "–û–∑–µ—Ä–æ", "–ü—Ä–∏—Ä–æ–¥–Ω—ã–π –æ–±—ä–µ–∫—Ç", "–¢—É—Ä–∏–∑–º / –û—Ç–¥—ã—Ö"],
    "food_drink": ["–ö–∞—Ñ–µ", "–†–µ—Å—Ç–æ—Ä–∞–Ω", "–ß–∞–π—Ö–∞–Ω–∞", "–°—Ç–æ–ª–æ–≤–∞—è", "–§–∞—Å—Ç—Ñ—É–¥", "–ü–∏—Ü—Ü–∞", "–û–±—â–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ –ø–∏—Ç–∞–Ω–∏–µ"],
    "kids_family": ["–î–µ—Ç—Å–∫–∏–π —Å–∞–¥", "–¶–µ–Ω—Ç—Ä —Ä–∞–∑–≤–∏—Ç–∏—è", "–°–µ–º–µ–π–Ω—ã–π —Ü–µ–Ω—Ç—Ä", "–®–∫–æ–ª–∞", "–ò–≥—Ä–æ–≤–∞—è –ø–ª–æ—â–∞–¥–∫–∞"],
    "health_wellness": ["–°–ø–∞", "–°–∞–ª–æ–Ω –∫—Ä–∞—Å–æ—Ç—ã", "–ú–∞—Å—Å–∞–∂", "–§–∏—Ç–Ω–µ—Å", "–ô–æ–≥–∞", "–ö—Ä–∞—Å–æ—Ç–∞ / –ó–¥–æ—Ä–æ–≤—å–µ"],
    "shopping": ["–ú–∞–≥–∞–∑–∏–Ω", "–°—É–ø–µ—Ä–º–∞—Ä–∫–µ—Ç", "–ë—É—Ç–∏–∫", "–¢–æ—Ä–≥–æ–≤—ã–π —Ü–µ–Ω—Ç—Ä"],
    "tourism": ["–û—Ç–µ–ª—å", "–•–æ—Å—Ç–µ–ª", "–ì–æ—Å—Ç–∏–Ω–∏—Ü–∞", "–¢—É—Ä–∏–∑–º / –û—Ç–¥—ã—Ö"]
}

# === –°—Ü–µ–Ω–∞—Ä–∏–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è ===
SCENARIO_KEYWORDS = {
    "–ü–∞—Ä–∫ —Å –ø—Ä–∏—Ä–æ–¥–æ–π –¥–ª—è –æ—Ç–¥—ã—Ö–∞": ["–ø–∞—Ä–∫", "–ø—Ä–∏—Ä–æ–¥–∞", "–æ–∑–µ—Ä–æ", "—Ä–µ–∫–∞", "—Å–∞–¥", "–∑–µ–ª–µ–Ω—å", "–ø—Ä–æ–≥—É–ª–∫–∞", "–æ—Ç–¥—ã—Ö", "–Ω–∞–±–µ—Ä–µ–∂–Ω–∞—è"],
    "–ê–∫—Ç–∏–≤–Ω—ã–π –¥–µ–Ω—å": ["—Å–ø–æ—Ä—Ç", "—Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞", "–∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å", "–ø–æ—Ö–æ–¥", "—Å–∫–µ–π—Ç", "–±–∞—Å–∫–µ—Ç–±–æ–ª", "–ø–∞—Ä–∫", "–∞—Ç—Ç—Ä–∞–∫—Ü–∏–æ–Ω"],
    "–°–µ–º–µ–π–Ω—ã–π –¥–µ–Ω—å": ["–¥–µ—Ç", "—Å–µ–º—å—è", "—Ä–∞–∑–≤–ª–µ—á–µ–Ω–∏—è", "–∏–≥—Ä–æ–≤–∞—è", "–∑–æ–æ–ø–∞—Ä–∫", "—Ü–µ–Ω—Ç—Ä", "—Ä–∞–∑–≤–∏—Ç–∏–µ"],
    "–ö—É–ª—å—Ç—É—Ä–Ω—ã–π –≤—ã—Ö–æ–¥–Ω–æ–π": ["–º—É–∑–µ–π", "—Ç–µ–∞—Ç—Ä", "–≥–∞–ª–µ—Ä–µ—è", "–∫–∏–Ω–æ—Ç–µ–∞—Ç—Ä", "–∏—Å—Ç–æ—Ä–∏—è", "—ç–∫—Å–∫—É—Ä—Å–∏—è"],
    "–ù–æ—á–Ω–∞—è –∂–∏–∑–Ω—å": ["–±–∞—Ä", "–∫–∞—Ä–∞–æ–∫–µ", "–ø–∞–±", "–Ω–æ—á–Ω–æ–π –∫–ª—É–±", "–≤–µ—á–µ—Ä–∏–Ω–∫–∞", "–¥—Ä—É–∑—å—è", "—Ä–∞–∑–≤–ª–µ—á–µ–Ω–∏—è"],
    "–ï–¥–∞ –∏ –Ω–∞–ø–∏—Ç–∫–∏": ["–∫–∞—Ñ–µ", "—Ä–µ—Å—Ç–æ—Ä–∞–Ω", "—á–∞–π—Ö–∞–Ω–∞", "—Ñ–∞—Å—Ç—Ñ—É–¥", "–µ–¥–∞", "–∫–æ—Ñ–µ", "–ø–∏—Ü—Ü–∞", "–¥–µ—Å–µ—Ä—Ç"]
}


# === –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –Ω–∞–º–µ—Ä–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è ===
def classify_intent(user_query: str) -> str:
    labels = list(INTENT_CATEGORY_MAP.keys())
    result = intent_classifier(user_query, candidate_labels=labels)
    return result["labels"][0] if result["labels"] else "general"


# === –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –Ω–∞–º–µ—Ä–µ–Ω–∏—é ===
def filter_by_intent(df, intent):
    if intent not in INTENT_CATEGORY_MAP:
        return df
    allowed = INTENT_CATEGORY_MAP[intent]
    mask = df["category"].apply(lambda x: any(cat.lower() in str(x).lower() for cat in allowed))
    filtered = df[mask]
    return filtered if not filtered.empty else df


# === –°—Ü–µ–Ω–∞—Ä–Ω–æ–µ –≤–∑–≤–µ—à–∏–≤–∞–Ω–∏–µ ===
def apply_scenario_weighting(df, user_type):
    """–£–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç –æ—Ü–µ–Ω–∫—É —Å—Ö–æ–∂–µ—Å—Ç–∏, –µ—Å–ª–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏—è —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å –∫–ª—é—á–µ–≤—ã–º–∏ —Å–ª–æ–≤–∞–º–∏ —Å—Ü–µ–Ω–∞—Ä–∏—è."""
    if user_type not in SCENARIO_KEYWORDS:
        return df

    keywords = SCENARIO_KEYWORDS[user_type]
    df["scenario_boost"] = df["category"].apply(
        lambda x: 1.2 if any(k.lower() in str(x).lower() for k in keywords) else 1.0
    )
    df["similarity"] = df["similarity"] * df["scenario_boost"]
    return df


# === –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ ===
def recommend_places(query, user_lat, user_lon, df, model, index, radius_km=5, top_k=10):
    from sklearn.metrics.pairwise import cosine_similarity

    query_vector = model.encode([query])
    vectors = df["embedding"].tolist()
    similarities = cosine_similarity(query_vector, vectors)[0]
    df["similarity"] = similarities

    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ —Ä–∞–¥–∏—É—Å—É
    df["dist_km"] = np.sqrt((df["latitude"] - user_lat) ** 2 + (df["longitude"] - user_lon) ** 2) * 111
    nearby = df[df["dist_km"] <= radius_km].copy()

    top_results = nearby.sort_values("similarity", ascending=False).head(top_k)
    return top_results


# === –†–µ—Ä–∞–Ω–∫–∏–Ω–≥ ===
def rerank_results(query, results_df):
    pairs = [(query, row["description"]) for _, row in results_df.iterrows()]
    scores = cross_encoder.predict(pairs)
    results_df["rerank_score"] = scores
    return results_df.sort_values("rerank_score", ascending=False)


# === LLM —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è ===
def llm_filter(query, user_type, results_df):
    scenario_context = f"–°—Ü–µ–Ω–∞—Ä–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {user_type}. "
    if user_type in SCENARIO_KEYWORDS:
        scenario_context += "–ö–ª—é—á–µ–≤—ã–µ —Ç–µ–º—ã: " + ", ".join(SCENARIO_KEYWORDS[user_type]) + "."

    context = "\n".join([
        f"{row['name']} ‚Äî {row.get('description', '–ë–µ–∑ –æ–ø–∏—Å–∞–Ω–∏—è')} "
        f"(–∫–∞—Ç–µ–≥–æ—Ä–∏—è: {row.get('category', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}, {row.get('dist_km', 0):.1f} –∫–º)"
        for _, row in results_df.iterrows()
    ])

    prompt = (
        f"–¢—ã ‚Äî —É–º–Ω—ã–π –≥–æ—Ä–æ–¥—Å–∫–æ–π –ø–æ–º–æ—â–Ω–∏–∫ –ø–æ –ê—Å—Ç–∞–Ω–µ.\n"
        f"–ó–∞–ø—Ä–æ—Å: '{query}'\n"
        f"{scenario_context}\n\n"
        f"–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å–ø–∏—Å–æ–∫ –Ω–∏–∂–µ –∏ –≤—ã–±–µ—Ä–∏ —Ç–æ–ª—å–∫–æ —Ç–µ –º–µ—Å—Ç–∞, –∫–æ—Ç–æ—Ä—ã–µ –ª—É—á—à–µ –≤—Å–µ–≥–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç —Å—Ü–µ–Ω–∞—Ä–∏—é –∏ –∑–∞–ø—Ä–æ—Å—É.\n"
        f"–°–ø–∏—Å–æ–∫ –º–µ—Å—Ç:\n{context}\n\n"
        f"–û—Ç–≤–µ—Ç—å —Ç–æ–ª—å–∫–æ —Å–ø–∏—Å–∫–æ–º –Ω–∞–∑–≤–∞–Ω–∏–π —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é –±–µ–∑ –ø–æ—è—Å–Ω–µ–Ω–∏–π."
    )

    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.3,
            max_tokens=300
        )
        content = response.choices[0].message.content.strip()
        names = [n.strip() for n in content.split(",") if n.strip()]
        filtered_df = results_df[results_df["name"].isin(names)]
        return filtered_df if not filtered_df.empty else results_df
    except Exception as e:
        print(f"‚ö† –û—à–∏–±–∫–∞ LLM-—Ñ–∏–ª—å—Ç—Ä–∞: {e}")
        return results_df


# === –≠–º–æ–¥–∑–∏ –∏ –æ–±—ä—è—Å–Ω–µ–Ω–∏—è ===
def explain_recommendations(results_df, user_type):
    explanations = []
    for _, row in results_df.iterrows():
        cat = (row.get("category", "") + " " + row.get("subcategory", "") + " " + row.get("description", "")).lower()

        if any(k in cat for k in ["–∫–∞—Ñ–µ", "—Ä–µ—Å—Ç–æ—Ä–∞–Ω", "–±–∞—Ä", "–ø–∏—Ü—Ü–∞", "–µ–¥–∞", "–∫–æ—Ñ–µ", "—á–∞–π—Ö–∞–Ω–∞", "—Ñ–∞—Å—Ç—Ñ—É–¥"]):
            emoji = "‚òï"
        elif any(k in cat for k in ["–Ω–æ—á–Ω", "–ø–∞–±", "–≤–µ—á–µ—Ä–∏–Ω", "–∫–∞—Ä–∞–æ–∫–µ", "–¥–∏—Å–∫–æ—Ç–µ–∫", "—Ä–∞–∑–≤–ª–µ—á"]):
            emoji = "üåô"
        elif any(k in cat for k in ["–ø–∞—Ä–∫", "—Å–∞–¥", "–ø—Ä–∏—Ä–æ–¥", "–æ–∑–µ—Ä–æ", "–æ—Ç–¥—ã—Ö", "—Ç—É—Ä–∏–∑–º"]):
            emoji = "üåø"
        elif any(k in cat for k in ["–¥–µ—Ç", "—à–∫–æ–ª", "—Å–µ–º–µ–π", "—Ä–∞–∑–≤–∏—Ç", "–∏–≥—Ä–æ–≤"]):
            emoji = "üë∂"
        elif any(k in cat for k in ["–º–∞–≥–∞–∑–∏–Ω", "—Ç–æ—Ä–≥–æ–≤", "—Ä—ã–Ω–æ–∫", "–±—É—Ç–∏–∫", "—Å—É–ø–µ—Ä–º–∞—Ä–∫–µ—Ç"]):
            emoji = "üõçÔ∏è"
        elif any(k in cat for k in ["—Å–ø–æ—Ä—Ç", "—Ñ–∏—Ç–Ω–µ—Å", "–π–æ–≥–∞", "–ø–ª–∞–≤–∞–Ω", "—Ç—Ä–µ–Ω–∞–∂–µ—Ä"]):
            emoji = "üèÄ"
        elif any(k in cat for k in ["–º–∞—Å—Å–∞–∂", "—Å–ø–∞", "–∫—Ä–∞—Å–æ—Ç–∞", "–∑–¥–æ—Ä–æ–≤—å–µ"]):
            emoji = "üíÜ"
        else:
            emoji = "üìç"

        explanations.append(
            f"{row['name']} ‚Äî {row.get('category', '–ë–µ–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏')} "
            f"({row['dist_km']:.2f} –∫–º –æ—Ç –≤–∞—Å) {emoji} ‚Äî –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ–µ –º–µ—Å—Ç–æ –ø–æ–±–ª–∏–∑–æ—Å—Ç–∏."
        )

    return explanations


# === –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è ===
def generate_smart_recommendations(query, user_lat, user_lon, df, model, index, user_type, radius_km=5, top_k=10):
    intent = classify_intent(query)
    df_filtered = filter_by_intent(df, intent)

    base_results = recommend_places(query, user_lat, user_lon, df_filtered, model, index, radius_km, top_k * 2)
    base_results = apply_scenario_weighting(base_results, user_type)  # <‚Äî –¥–æ–±–∞–≤–ª–µ–Ω–æ

    reranked = rerank_results(query, base_results)
    filtered = llm_filter(query, user_type, reranked.head(top_k))

    return filtered.head(top_k)
